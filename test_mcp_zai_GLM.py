#!/usr/bin/env python3
"""
MCP Z.AI GLM-4.6V PDF Parser Test

This script takes a PDF file, renders it to an image, and uses GLM-4.6V
via MCP to extract structured content in SimpleDocument format.

Usage:
    python test_mcp_zai_GLM.py path/to/document.pdf
    claude -p test_mcp_zai_GLM.py path/to/document.pdf

The script will:
1. Render the PDF page to a PNG image
2. Call GLM-4.6V via MCP to analyze the image
3. Parse the response into SimpleDocument format
4. Display and save the results
"""

import sys
import json
import base64
from pathlib import Path
from datetime import datetime
import argparse

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent))

from vlm_doc_test.schemas.schema_simple import (
    SimpleDocument,
    DocumentMetadata,
    DocumentSource,
    ContentElement,
    Table,
    Figure,
    Link,
    Author,
)
from vlm_doc_test.schemas.base import DocumentFormat, DocumentCategory


def render_pdf_to_image(pdf_path: Path, page: int = 0, dpi: int = 150) -> Path:
    """
    Render a PDF page to a PNG image.

    Args:
        pdf_path: Path to PDF file
        page: Page number to render (0-indexed)
        dpi: Resolution for rendering

    Returns:
        Path to the generated PNG image
    """
    import pymupdf as fitz

    print(f"üìÑ Rendering PDF: {pdf_path.name}")
    print(f"   Page: {page + 1}")
    print(f"   DPI: {dpi}")

    # Open PDF
    doc = fitz.open(pdf_path)

    if page >= len(doc):
        raise ValueError(f"Page {page} does not exist (PDF has {len(doc)} pages)")

    # Get the page
    page_obj = doc[page]

    # Render to pixmap at specified DPI
    mat = fitz.Matrix(dpi / 72, dpi / 72)  # 72 is default DPI
    pix = page_obj.get_pixmap(matrix=mat)

    # Save as PNG
    output_path = pdf_path.parent / f"{pdf_path.stem}_page{page}.png"
    pix.save(str(output_path))

    doc.close()

    print(f"‚úÖ Rendered to: {output_path}")
    print(f"   Size: {output_path.stat().st_size / 1024:.1f} KB")
    print(f"   Dimensions: {pix.width}x{pix.height}")

    return output_path


def encode_image_to_base64(image_path: Path) -> str:
    """
    Encode image to base64 data URI.

    Args:
        image_path: Path to image file

    Returns:
        Data URI string (data:image/png;base64,...)
    """
    print(f"\nüîß Encoding image to base64...")

    with open(image_path, 'rb') as f:
        image_bytes = f.read()
        image_b64 = base64.b64encode(image_bytes).decode('utf-8')
        data_uri = f"data:image/png;base64,{image_b64}"

    print(f"‚úÖ Encoded: {len(data_uri)} characters")

    return data_uri


def build_extraction_prompt(category: DocumentCategory = None) -> str:
    """
    Build extraction prompt for GLM-4.6V based on document category.

    Args:
        category: Optional document category for specialized extraction

    Returns:
        Extraction prompt string
    """
    base_prompt = """Analyze this document image and extract ALL information in JSON format.

Extract the following fields:
- title: Document title (string)
- authors: List of author names (array of strings)
- date: Publication or creation date if visible (string)
- keywords: Keywords or tags (array of strings)
- abstract: Abstract or summary if present (string)
- content: ALL text content with structure (array of objects)
  Each content item should have:
  - type: "heading", "paragraph", "list", "code", or "quote"
  - text: The actual text content
  - level: For headings, the level (1-6)
- tables: ALL tables with their data (array of objects)
  Each table should have:
  - caption: Table caption or title
  - label: Table label (e.g., "Table 1")
  - rows: 2D array of cell values
- figures: ALL figures/images with captions (array of objects)
  Each figure should have:
  - caption: Figure caption
  - label: Figure label (e.g., "Figure 1")
- links: ALL visible links/URLs (array of objects)
  Each link should have:
  - url: The URL
  - text: Link text or anchor text

Return ONLY valid JSON in this EXACT format:
{
  "title": "string or null",
  "authors": ["string"],
  "date": "string or null",
  "keywords": ["string"],
  "abstract": "string or null",
  "content": [
    {"type": "heading", "text": "string", "level": 1},
    {"type": "paragraph", "text": "string"},
    {"type": "list", "text": "string"}
  ],
  "tables": [
    {
      "caption": "string or null",
      "label": "string or null",
      "rows": [["cell1", "cell2"], ["cell3", "cell4"]]
    }
  ],
  "figures": [
    {
      "caption": "string or null",
      "label": "string or null"
    }
  ],
  "links": [
    {
      "url": "string",
      "text": "string"
    }
  ]
}

IMPORTANT: Extract ALL content, not just summaries. Be thorough and complete.
"""

    # Add category-specific instructions
    if category == DocumentCategory.ACADEMIC_PAPER:
        base_prompt += """

DOCUMENT TYPE: ACADEMIC PAPER
Pay special attention to:
- Author names and affiliations
- Abstract section
- Section structure (Introduction, Methods, Results, Discussion, etc.)
- Figure and table captions with numbers
- Citations and references
- Mathematical equations or formulas
"""
    elif category == DocumentCategory.BLOG_POST:
        base_prompt += """

DOCUMENT TYPE: BLOG POST
Pay special attention to:
- Author name and bio
- Publication date
- Tags or categories
- Main content paragraphs
- Embedded links
- Comments section if visible
"""
    elif category == DocumentCategory.TECHNICAL_DOCUMENTATION:
        base_prompt += """

DOCUMENT TYPE: TECHNICAL DOCUMENTATION
Pay special attention to:
- API names and function signatures
- Code blocks and examples
- Parameter descriptions
- Return values
- Navigation links
- Version information
"""
    elif category == DocumentCategory.NEWS_ARTICLE:
        base_prompt += """

DOCUMENT TYPE: NEWS ARTICLE
Pay special attention to:
- Headline
- Byline (author)
- Publication date and time
- Location/dateline
- Main story content
- Related links
"""

    return base_prompt


def call_mcp_tool(image_uri: str, prompt: str) -> str:
    """
    Call the MCP image_analysis tool.

    This function attempts to call the actual MCP tool if available,
    otherwise provides instructions for manual execution.

    Args:
        image_uri: Base64-encoded image data URI
        prompt: Extraction prompt

    Returns:
        JSON string response from GLM-4.6V
    """
    print(f"\nü§ñ Calling GLM-4.6V via MCP...")
    print(f"   Tool: mcp__zai_mcp_server__image_analysis")
    print(f"   Image size: {len(image_uri)} characters")
    print(f"   Prompt size: {len(prompt)} characters")

    # NOTE: When running with `claude -p`, the MCP tool would be available
    # For now, we'll simulate the call and provide instructions

    try:
        # This is where the actual MCP tool call would happen
        # In Claude Code environment with MCP enabled:
        # result = call_mcp_tool_internal("mcp__zai_mcp_server__image_analysis",
        #                                  image=image_uri, prompt=prompt)

        # For now, raise NotImplementedError to show what would happen
        raise NotImplementedError("MCP tool call")

    except NotImplementedError:
        print("\n" + "="*70)
        print("‚ö†Ô∏è  MCP Tool Not Available in Standard Python")
        print("="*70)
        print("\nTo make actual GLM-4.6V API calls, run with Claude Code:")
        print(f"  $ claude -p {sys.argv[0]} [pdf_file]")
        print("\nFor now, showing simulated response structure...")
        print("="*70)

        # Return a simulated response structure
        simulated_response = {
            "title": "Sample Document Title",
            "authors": ["Author from PDF"],
            "date": None,
            "keywords": [],
            "abstract": None,
            "content": [
                {
                    "type": "heading",
                    "text": "Document content would be extracted here",
                    "level": 1
                },
                {
                    "type": "paragraph",
                    "text": "This is a simulated response. To get real extraction results, run with: claude -p test_mcp_zai_GLM.py your_file.pdf"
                }
            ],
            "tables": [],
            "figures": [],
            "links": []
        }

        return json.dumps(simulated_response)


def parse_vlm_response(response_json: str, source_path: Path, category: DocumentCategory = None) -> SimpleDocument:
    """
    Parse GLM-4.6V JSON response into SimpleDocument format.

    Args:
        response_json: JSON string from GLM-4.6V
        source_path: Original PDF file path
        category: Document category

    Returns:
        SimpleDocument object
    """
    print(f"\nüìã Parsing GLM-4.6V response...")

    # Parse JSON
    try:
        data = json.loads(response_json)
    except json.JSONDecodeError as e:
        print(f"‚ùå JSON parsing error: {e}")
        # Try to extract JSON from text
        import re
        json_match = re.search(r'\{.*\}', response_json, re.DOTALL)
        if json_match:
            data = json.loads(json_match.group())
        else:
            raise ValueError("Could not parse VLM response as JSON")

    # Extract metadata
    metadata = DocumentMetadata(
        title=data.get("title"),
        authors=[Author(name=name) for name in data.get("authors", [])],
        date=data.get("date"),
        keywords=data.get("keywords", []),
    )

    # Add abstract if present
    if data.get("abstract"):
        # Store abstract as a custom field
        metadata.__dict__['abstract'] = data["abstract"]

    # Extract content elements
    content = []
    for idx, item in enumerate(data.get("content", [])):
        element = ContentElement(
            id=f"vlm_c{idx}",
            type=item.get("type", "paragraph"),
            content=item.get("text", ""),
            level=item.get("level"),
        )
        content.append(element)

    # Extract tables
    tables = []
    for idx, table_data in enumerate(data.get("tables", [])):
        table = Table(
            id=f"vlm_t{idx}",
            caption=table_data.get("caption"),
            label=table_data.get("label"),
            rows=table_data.get("rows", []),
        )
        tables.append(table)

    # Extract figures
    figures = []
    for idx, fig_data in enumerate(data.get("figures", [])):
        figure = Figure(
            id=f"vlm_f{idx}",
            caption=fig_data.get("caption"),
            label=fig_data.get("label"),
        )
        figures.append(figure)

    # Extract links
    links = []
    for idx, link_data in enumerate(data.get("links", [])):
        link = Link(
            id=f"vlm_l{idx}",
            url=link_data.get("url", ""),
            text=link_data.get("text", ""),
        )
        links.append(link)

    # Create SimpleDocument
    document = SimpleDocument(
        id=f"vlm_{source_path.stem}",
        format=DocumentFormat.PDF,
        category=category,
        source=DocumentSource(
            file_path=str(source_path),
            accessed_at=datetime.now(),
        ),
        metadata=metadata,
        content=content,
        tables=tables,
        figures=figures,
        links=links,
    )

    print(f"‚úÖ Parsed SimpleDocument:")
    print(f"   Title: {document.metadata.title}")
    print(f"   Authors: {len(document.metadata.authors)}")
    print(f"   Content elements: {len(document.content)}")
    print(f"   Tables: {len(document.tables)}")
    print(f"   Figures: {len(document.figures)}")
    print(f"   Links: {len(document.links)}")

    return document


def save_results(document: SimpleDocument, output_path: Path):
    """
    Save extraction results to JSON file.

    Args:
        document: SimpleDocument object
        output_path: Path to save JSON file
    """
    print(f"\nüíæ Saving results to: {output_path}")

    # Convert to dict
    doc_dict = document.model_dump(mode='json')

    # Save as JSON
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(doc_dict, f, indent=2, ensure_ascii=False)

    print(f"‚úÖ Saved: {output_path.stat().st_size / 1024:.1f} KB")


def display_results(document: SimpleDocument):
    """
    Display extraction results in a readable format.

    Args:
        document: SimpleDocument object
    """
    print("\n" + "="*70)
    print("  EXTRACTION RESULTS (SimpleDocument Format)")
    print("="*70)

    # Metadata
    print("\nüìä METADATA:")
    print(f"  ID: {document.id}")
    print(f"  Format: {document.format.value}")
    print(f"  Category: {document.category.value if document.category else 'None'}")
    print(f"  Title: {document.metadata.title or 'N/A'}")
    print(f"  Authors: {', '.join([a.name for a in document.metadata.authors]) or 'N/A'}")
    if document.metadata.date:
        print(f"  Date: {document.metadata.date}")
    if document.metadata.keywords:
        print(f"  Keywords: {', '.join(document.metadata.keywords)}")
    if hasattr(document.metadata, 'abstract') and document.metadata.__dict__.get('abstract'):
        print(f"  Abstract: {document.metadata.__dict__['abstract'][:100]}...")

    # Content
    print(f"\nüìù CONTENT ({len(document.content)} elements):")
    for i, element in enumerate(document.content[:10]):  # Show first 10
        prefix = f"  [{i+1}]"
        if element.type == "heading":
            print(f"{prefix} HEADING (level {element.level}): {element.content}")
        elif element.type == "paragraph":
            preview = element.content[:80] + "..." if len(element.content) > 80 else element.content
            print(f"{prefix} PARAGRAPH: {preview}")
        else:
            preview = element.content[:80] + "..." if len(element.content) > 80 else element.content
            print(f"{prefix} {element.type.upper()}: {preview}")

    if len(document.content) > 10:
        print(f"  ... and {len(document.content) - 10} more elements")

    # Tables
    if document.tables:
        print(f"\nüìä TABLES ({len(document.tables)}):")
        for i, table in enumerate(document.tables):
            print(f"  [{i+1}] {table.label or 'Table'}: {table.caption or 'No caption'}")
            print(f"      Size: {len(table.rows)} rows x {len(table.rows[0]) if table.rows else 0} columns")

    # Figures
    if document.figures:
        print(f"\nüñºÔ∏è  FIGURES ({len(document.figures)}):")
        for i, figure in enumerate(document.figures):
            print(f"  [{i+1}] {figure.label or 'Figure'}: {figure.caption or 'No caption'}")

    # Links
    if document.links:
        print(f"\nüîó LINKS ({len(document.links)}):")
        for i, link in enumerate(document.links[:5]):  # Show first 5
            print(f"  [{i+1}] {link.text or 'Link'}: {link.url}")
        if len(document.links) > 5:
            print(f"  ... and {len(document.links) - 5} more links")

    print("\n" + "="*70)


def main():
    """Main function."""
    parser = argparse.ArgumentParser(
        description='Extract structured content from PDF using GLM-4.6V via MCP',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Process a PDF file
  python test_mcp_zai_GLM.py research_paper.pdf

  # Process specific page with category
  python test_mcp_zai_GLM.py document.pdf --page 2 --category academic_paper

  # Use with Claude Code for real MCP calls
  claude -p test_mcp_zai_GLM.py paper.pdf

Categories:
  academic_paper, blog_post, news_article, technical_documentation,
  book_chapter, presentation, report, tutorial, webpage
        """
    )

    parser.add_argument('pdf_file', type=Path, help='Path to PDF file')
    parser.add_argument('--page', type=int, default=0, help='Page number to process (0-indexed, default: 0)')
    parser.add_argument('--dpi', type=int, default=150, help='DPI for PDF rendering (default: 150)')
    parser.add_argument('--category', type=str, help='Document category for specialized extraction')
    parser.add_argument('--output', type=Path, help='Output JSON file path (default: [pdf_name]_extracted.json)')
    parser.add_argument('--keep-image', action='store_true', help='Keep the rendered PNG image')

    args = parser.parse_args()

    # Validate PDF file
    if not args.pdf_file.exists():
        print(f"‚ùå Error: File not found: {args.pdf_file}")
        return 1

    if args.pdf_file.suffix.lower() != '.pdf':
        print(f"‚ùå Error: File must be a PDF: {args.pdf_file}")
        return 1

    # Parse category
    category = None
    if args.category:
        try:
            category = DocumentCategory(args.category.lower())
        except ValueError:
            print(f"‚ùå Error: Invalid category: {args.category}")
            print(f"Valid categories: {', '.join([c.value for c in DocumentCategory])}")
            return 1

    # Set output path
    output_path = args.output or args.pdf_file.parent / f"{args.pdf_file.stem}_extracted.json"

    print("="*70)
    print("  GLM-4.6V PDF CONTENT EXTRACTION via MCP")
    print("="*70)
    print(f"\nüìÑ Input: {args.pdf_file}")
    print(f"üìÑ Page: {args.page + 1}")
    print(f"üìÅ Output: {output_path}")
    if category:
        print(f"üìÇ Category: {category.value}")
    print(f"‚è∞ Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    try:
        # Step 1: Render PDF to image
        image_path = render_pdf_to_image(args.pdf_file, page=args.page, dpi=args.dpi)

        # Step 2: Encode image to base64
        image_uri = encode_image_to_base64(image_path)

        # Step 3: Build extraction prompt
        prompt = build_extraction_prompt(category)
        print(f"\nüìù Built extraction prompt ({len(prompt)} characters)")
        if category:
            print(f"   Optimized for: {category.value}")

        # Step 4: Call MCP tool
        response_json = call_mcp_tool(image_uri, prompt)

        # Step 5: Parse response into SimpleDocument
        document = parse_vlm_response(response_json, args.pdf_file, category)

        # Step 6: Save results
        save_results(document, output_path)

        # Step 7: Display results
        display_results(document)

        # Cleanup
        if not args.keep_image:
            image_path.unlink()
            print(f"\nüóëÔ∏è  Removed temporary image: {image_path.name}")
        else:
            print(f"\nüíæ Kept image: {image_path}")

        print("\n" + "="*70)
        print("  ‚úÖ SUCCESS!")
        print("="*70)
        print(f"\nüìÅ Results saved to: {output_path}")
        print(f"‚è∞ Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        print("\nüí° To use real GLM-4.6V extraction (not simulation):")
        print(f"   claude -p {sys.argv[0]} {args.pdf_file}")

        return 0

    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
